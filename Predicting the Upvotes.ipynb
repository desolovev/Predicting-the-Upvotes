{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Upvotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll be predicting the number of upvotes the articles received, based on their headlines. Because upvotes are an indicator of popularity, we'll discover which types of articles tend to be the most popular.\n",
    "\n",
    "Hacker News is a community where users can submit articles, and other users can upvote those articles. The articles with the most upvotes make it to the front page, where they're more visible to the community. Our data set consists of submissions users made to Hacker News from 2006 to 2015.\n",
    "\n",
    "Our data only has four columns:-\n",
    "\n",
    "- **submission_time** - When the article was submitted\n",
    "- **upvotes** - The number of upvotes the article received\n",
    "- **url** - The base URL of the article\n",
    "- **headline** - The article's headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(\"sel_hn_stories.csv\")\n",
    "submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2999 entries, 0 to 2998\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   submission_time  2999 non-null   object\n",
      " 1   upvotes          2999 non-null   int64 \n",
      " 2   url              2810 non-null   object\n",
      " 3   headline         2989 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "submissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2800 entries, 0 to 2998\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   submission_time  2800 non-null   object\n",
      " 1   upvotes          2800 non-null   int64 \n",
      " 2   url              2800 non-null   object\n",
      " 3   headline         2800 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 109.4+ KB\n"
     ]
    }
   ],
   "source": [
    "submissions = submissions.dropna()\n",
    "submissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to train a linear regression algorithm that predicts the number of upvotes a headline would receive. To do this, we'll need to convert each headline to a numerical representation.\n",
    "\n",
    "For this we will convert the headings into a list of words and cleaning the list (removing the punctuations) and finally make a list of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headlines = []\n",
    "\n",
    "for row in submissions['headline']:\n",
    "    lst = row.split()\n",
    "    tokenized_headlines.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "\n",
    "for item in tokenized_headlines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuation:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making `columns` of total each unique words and `rows` equal to total number of headings we have. It contains the occurence of word in headings. Like this\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "<h4> 1) [i, rode, my, horse, to, berlin]<h4/>\n",
    "<center>2) [you, rode, my, horse, to, berlin, in, the, winter]\n",
    "\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "<center>   [i, rode, my, horse, to, berlin, in, the, winter, you]\n",
    "   \n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "    ↓                       ↓\n",
    "<center/>\n",
    "   \n",
    "  |   | i | rode | my | horse | to | berlin | in | the | winter | you |\n",
    "|---|---|------|----|-------|----|--------|----|-----|--------|-----|\n",
    "| 1 | 1 | 1    | 1  | 1     | 1  | 1      | 0  | 0   | 0      | 0   |\n",
    "| 2 | 0 | 1    | 1  | 1     | 1  | 1      | 1  | 1   | 1      | 1   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "\n",
    "for each in clean_tokenized:\n",
    "    for item in each:\n",
    "        if item not in single_tokens:\n",
    "            single_tokens.append(item)\n",
    "        elif item in single_tokens and item not in unique_tokens:\n",
    "            unique_tokens.append(item)\n",
    "            \n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, each in enumerate(clean_tokenized):\n",
    "    for item in each:\n",
    "        if item in unique_tokens:\n",
    "            counts.iloc[index][item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2800 entries, 0 to 2799\n",
      "Columns: 2310 entries, and to disaster\n",
      "dtypes: int64(2310)\n",
      "memory usage: 49.4 MB\n"
     ]
    }
   ],
   "source": [
    "counts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have over 2000 columns in our matrix. This can make it very hard for a linear regression model to make good predictions. Too many columns will cause the model to fit to noise instead of the signal in the data.\n",
    "\n",
    "There are two kinds of features that will reduce prediction accuracy. Features that occur only a few times will cause overfitting, because the model doesn't have enough information to accurately decide whether they're important. These features will probably correlate differently with upvotes in the test set and the training set.\n",
    "\n",
    "Features that occur too many times can also cause issues. These are words like `and` and `to`, which occur in nearly every headline. These words don't add any information, because they don't necessarily correlate with upvotes. These types of words are sometimes called `stopwords`.\n",
    "\n",
    "To reduce the number of features and enable the linear regression model to make better predictions, we'll remove any words that occur fewer than `5` times or more than `100` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = counts.sum(axis=0)\n",
    "counts = counts.loc[:,(word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to split the data into two sets so that we can evaluate our algorithm effectively. We'll train our algorithm on a training set, then test its performance on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a training set and a test set, let's train a model and make test predictions. When we make predictions with a linear regression model, the model assigns coefficients to each column. \n",
    "\n",
    "Essentially, the model is determining which words correlate with more upvotes, and which with less. By finding these correlations, the model will be able to predict which headlines will be highly upvoted in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.1457056689665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with `RandomForestRegression` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(max_depth=20, random_state=1)\n",
    "rfr.fit(X_train, y_train)\n",
    "predictions = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039.3099402428354\n"
     ]
    }
   ],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it shows some improvement. It may possible that it will be overfited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
